{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuM6s-1bCwXW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roVRIKuFGRm7",
    "outputId": "32f3a59c-da36-4032-de86-147ab2d82692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A pasta ./dataset.zip foi descompactada em ./.\n"
     ]
    }
   ],
   "source": [
    "caminho_zip = './dataset.zip'\n",
    "diretorio_destino = './'\n",
    "\n",
    "# Certifique-se de que o diretório de destino exista ou crie-o\n",
    "if not os.path.exists(diretorio_destino):\n",
    "    os.makedirs(diretorio_destino)\n",
    "\n",
    "# Descompactar o arquivo .zip\n",
    "with zipfile.ZipFile(caminho_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(diretorio_destino)\n",
    "\n",
    "print(f'A pasta {caminho_zip} foi descompactada em {diretorio_destino}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhIykF0tGTF5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('eng-por.txt', sep='\\t')\n",
    "df.columns = ['Ingles', 'Portugues', 'descatar']\n",
    "df = df.drop(columns=['descatar'])\n",
    "df.columns = ['', '']\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv('dataset.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Lcy9LSpGTT8",
    "outputId": "90e1c025-5425-41cc-9cac-b4357e035ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom seemed glad to see us again. => Tom parecia feliz em nos ver novamente.\n",
      "Control is everything. => Controle é tudo.\n",
      "We are not Americans. => Nós não somos americanos.\n"
     ]
    }
   ],
   "source": [
    "path = str(os.getcwd())\n",
    "\n",
    "text = (Path(path) / \"dataset.txt\").read_text()\n",
    "\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "\n",
    "np.random.shuffle(pairs)\n",
    "\n",
    "sentences_en, sentences_es = zip(*pairs) # separates the pairs into 2 lists\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0siDHYQGTjR"
   },
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    " vocab_size, output_sequence_length=max_length)\n",
    "\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
    " vocab_size, output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xu4yBXoUGl0p",
    "outputId": "68d206c9-8da7-4086-9ac5-3d759b78da2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'startofseq', 'endofseq', 'tom', 'que', 'o', 'não', 'eu', 'de']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])\n",
    "text_vec_layer_en.get_vocabulary()[:10]\n",
    "text_vec_layer_es.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Am26sPDfGl9R"
   },
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "\n",
    "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBrUdquJGsDj"
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_HrkVMtGsM2"
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    " mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    " mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4C8x2TYCGsV_"
   },
   "outputs": [],
   "source": [
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1byaY67zGsd7"
   },
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "\n",
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2zXqga4DWXe"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"modelo_tradutor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzzyTM5QGsrj"
   },
   "outputs": [],
   "source": [
    "def translate(sentence_en):\n",
    "     translation = \"\"\n",
    "     for word_idx in range(max_length):\n",
    "         X = np.array([sentence_en]) # encoder input\n",
    "         X_dec = np.array([\"startofseq \" + translation]) # decoder input\n",
    "         y_proba = model.predict((X, X_dec))[0, word_idx] # last token's probas\n",
    "         predicted_word_id = np.argmax(y_proba)\n",
    "         predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "         if predicted_word == \"endofseq\":\n",
    "             break\n",
    "         translation += \" \" + predicted_word\n",
    "     return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "641af45e",
    "outputId": "fe113c76-124e-4390-ac92-c4fb9202f3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "gosto de comer\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "elas estão indo\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "meus filhos são bons\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "eu não gosto da minha família\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "eu não sei ler\n"
     ]
    }
   ],
   "source": [
    "vetor = ['I like to eat', 'They are going', 'My children is good', 'I dont like my family', 'I dont know read']\n",
    "\n",
    "for frase in vetor:\n",
    "    print(translate(frase))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
